{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "#import sklearn to use linear regression algorithm to build model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#import sklearn to use decision tree regression algorithm to build model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "#Import Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#Import Support Vector Machine\n",
    "from sklearn import svm\n",
    "\n",
    "#Import KNN\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsRegressor\n",
    "\n",
    "#Import ANN\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "#Import Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "#import sklearn to split dataset into train/test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import sklearn to perform k-fold cross validation and evaluation of model\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'MLR'\n",
    "\n",
    "model_formula = LinearRegression()\n",
    "# model_formula = KNeighborsRegressor(n_neighbors=4)\n",
    "# model_formula = RandomForestRegressor(n_estimators = 50, random_state = 42, criterion = 'mse', min_samples_split = 10, max_features = 8)\n",
    "# model_formula = GradientBoostingRegressor(loss='huber', learning_rate=0.1, n_estimators=200, min_samples_split=2, alpha=0.9)\n",
    "# model_formula = MLPRegressor(activation='tanh', solver='adam', alpha=0.0001, learning_rate_init=0.001, max_iter=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio Range\n",
    "start = 0.70\n",
    "end = 0.95\n",
    "increment = 0.02\n",
    "\n",
    "# Particle Size Range\n",
    "start_ = 0.4\n",
    "end_ = 1.3\n",
    "increment_ = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "color='C0'\n",
    "color_nd='C1'\n",
    "title_size = 14\n",
    "label_size = 12\n",
    "suptitle_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\sph0088\\\\OneDrive - UNT System\\\\AQ\\\\Calibration_Sensors\\\\Collocation_Data\\\\test_data\\\\sensor_tceq.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a21b25be3755>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'C:\\Users\\{}\\OneDrive - UNT System\\AQ\\Calibration_Sensors\\Collocation_Data\\test_data\\sensor_tceq.csv'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomputer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tceq_pm25'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pm2_5_atm_avg'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pm10_0_atm_avg'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#filter out abnormal values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    699\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    702\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\sph0088\\\\OneDrive - UNT System\\\\AQ\\\\Calibration_Sensors\\\\Collocation_Data\\\\test_data\\\\sensor_tceq.csv'"
     ]
    }
   ],
   "source": [
    "computer = 'sph0088'\n",
    "training = 'Set 1'\n",
    "val = 'Set 2'\n",
    "test = 'Set 3'\n",
    "test2 = 'Set 4'\n",
    "\n",
    "file = r'C:\\Users\\{}\\OneDrive - UNT System\\AQ\\Calibration_Sensors\\Collocation_Data\\test_data\\sensor_tceq.csv'.format(computer)\n",
    "df = pd.read_csv(file)\n",
    "df = df[df['tceq_pm25']>=0]\n",
    "df = df[(df['pm2_5_atm_avg'] < df['pm10_0_atm_avg'])] #filter out abnormal values\n",
    "df = df[(df['pm1_0_atm_avg'] < (df['pm2_5_atm_avg']*2.5))] #filter out abnormal values\n",
    "df['p03_p05'] = df['p_0_3_um_avg'] / df['p_0_5_um_avg']\n",
    "df = df[(df['p03_p05'] < 3.75)] #filter out abnormal values\n",
    "\n",
    "\n",
    "val_file = r'C:\\Users\\{}\\OneDrive - UNT System\\AQ\\Calibration_Sensors\\Collocation_Data\\test_data\\sensor_tceq_new_set2.csv'.format(computer)\n",
    "df_val = pd.read_csv(val_file)\n",
    "df_val = df_val[df_val['tceq_pm25']>=0]\n",
    "df_val = df_val[(df_val['pm2_5_atm_avg'] < df_val['pm10_0_atm_avg'])] #filter out abnormal values\n",
    "df_val = df_val[(df_val['pm1_0_atm_avg'] < (df_val['pm2_5_atm_avg']*2.5))] #filter out abnormal values\n",
    "df_val['p03_p05'] = df_val['p_0_3_um_avg'] / df_val['p_0_5_um_avg']\n",
    "df_val = df_val[(df_val['p03_p05'] < 3.75)] #filter out abnormal values\n",
    "df_val\n",
    "\n",
    "\n",
    "test_file = r'C:\\Users\\{}\\OneDrive - UNT System\\AQ\\Calibration_Sensors\\Collocation_Data\\test_data\\sensor_tceq_new_set3.csv'.format(computer)\n",
    "df_test = pd.read_csv(test_file)\n",
    "df_test = df_test[df_test['tceq_pm25']>=0]\n",
    "df_test = df_test[(df_test['pm2_5_atm_avg'] < df_test['pm10_0_atm_avg'])] #filter out abnormal values\n",
    "df_test = df_test[(df_test['pm1_0_atm_avg'] < (df_test['pm2_5_atm_avg']*2.5))] #filter out abnormal values\n",
    "df_test['p03_p05'] = df_test['p_0_3_um_avg'] / df_test['p_0_5_um_avg']\n",
    "df_test = df_test[(df_test['p03_p05'] < 3.75)] #filter out abnormal values\n",
    "df_test\n",
    "\n",
    "test2_file = r'C:\\Users\\{}\\OneDrive - UNT System\\AQ\\Calibration_Sensors\\Collocation_Data\\test_data\\sensor_tceq_new_set4.csv'.format(computer)\n",
    "df_test2 = pd.read_csv(test2_file)\n",
    "df_test2 = df_test2[df_test2['tceq_pm25']>=0]\n",
    "df_test2 = df_test2[(df_test2['pm2_5_atm_avg'] < df_test2['pm10_0_atm_avg'])] #filter out abnormal values\n",
    "df_test2 = df_test2[(df_test2['pm1_0_atm_avg'] < (df_test2['pm2_5_atm_avg']*2.5))] #filter out abnormal values\n",
    "df_test2['p03_p05'] = df_test2['p_0_3_um_avg'] / df_test2['p_0_5_um_avg']\n",
    "df_test2 = df_test2[(df_test2['p03_p05'] < 3.75)] #filter out abnormal values\n",
    "print('Set 1', df.shape)\n",
    "print('Set 2', df_val.shape)\n",
    "print('Set 3', df_test.shape)\n",
    "print('Set 4', df_test2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation (Set 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "################################## Edit each time change is made #####################################\n",
    "particle_size_val = []\n",
    "keys_ratio_val = []\n",
    "dust_nrmse_ratio_val = []\n",
    "nodust_nrmse_ratio_val = []\n",
    "nrmse_mean_ratio_val = []\n",
    "dust_rmse_ratio_val = []\n",
    "nodust_rmse_ratio_val = []\n",
    "rmse_mean_ratio_val = []\n",
    "dust_hours_ratio_val = []\n",
    "nodust_hours_ratio_val = []\n",
    "r2_mean_val = []\n",
    "dust_r2_val = []\n",
    "nodust_r2_val = []\n",
    "dust_mae_val = []\n",
    "nodust_mae_val = []\n",
    "mean_mae_val = []\n",
    "dust_nmae_val = []\n",
    "nodust_nmae_val = []\n",
    "mean_nmae_val = []\n",
    "\n",
    "################################## PA-PM2.5 Treshold ###########################################\n",
    "# pm25 = \n",
    "for p_size in np.arange(start_,end_,increment_):\n",
    "    for threshold in np.arange(start,end,increment):\n",
    "#################################### Dust Filters #############################################\n",
    "        df_dust = df[(df['p_10_0_um_avg'] > p_size) | (df['pm25_pm10'] < threshold)]\n",
    "        df_nodust = df[~((df['p_10_0_um_avg'] > p_size) | (df['pm25_pm10'] < threshold))]\n",
    "\n",
    "        df_dust_val = df_val[(df_val['p_10_0_um_avg'] > p_size) | (df_val['pm25_pm10'] < threshold)]\n",
    "        df_nodust_val = df_val[~((df_val['p_10_0_um_avg'] > p_size) | (df_val['pm25_pm10'] < threshold))]\n",
    "\n",
    "    ################################### Slice Datasets ##########################################################\n",
    "    #################################### Training Set ##############################################################\n",
    "        VG7 = df_dust[['current_temp_f', 'current_humidity'\n",
    "                       ,'p_0_3_um_avg', 'p_0_5_um_avg', 'p_1_0_um_avg'\n",
    "                       , 'p_2_5_um_avg','p_5_0_um_avg', 'p_10_0_um_avg'\n",
    "                        ,'pm2_5_atm_avg','pm1_0_atm_avg', 'pm10_0_atm_avg'\n",
    "                        ,'pm1_pm25', 'pm25_pm10', 'pm1_pm10']]\n",
    "\n",
    "        Y = df_dust['tceq_pm25']\n",
    "\n",
    "        VG7_nd = df_nodust[['current_temp_f', 'current_humidity'\n",
    "                       ,'p_0_3_um_avg', 'p_0_5_um_avg', 'p_1_0_um_avg'\n",
    "                       , 'p_2_5_um_avg','p_5_0_um_avg', 'p_10_0_um_avg'\n",
    "                        ,'pm2_5_atm_avg','pm1_0_atm_avg', 'pm10_0_atm_avg'\n",
    "                        ,'pm1_pm25', 'pm25_pm10', 'pm1_pm10']]\n",
    "\n",
    "        Y_nd = df_nodust['tceq_pm25']\n",
    "\n",
    "    #################################### Validation Set ##############################################################\n",
    "        VG7_val = df_dust_val[['current_temp_f', 'current_humidity'\n",
    "                       ,'p_0_3_um_avg', 'p_0_5_um_avg', 'p_1_0_um_avg'\n",
    "                       , 'p_2_5_um_avg','p_5_0_um_avg', 'p_10_0_um_avg'\n",
    "                        ,'pm2_5_atm_avg','pm1_0_atm_avg', 'pm10_0_atm_avg'\n",
    "                        ,'pm1_pm25', 'pm25_pm10', 'pm1_pm10']]\n",
    "\n",
    "        Y_val = df_dust_val['tceq_pm25']\n",
    "\n",
    "        VG7_nd_val = df_nodust_val[['current_temp_f', 'current_humidity'\n",
    "                       ,'p_0_3_um_avg', 'p_0_5_um_avg', 'p_1_0_um_avg'\n",
    "                       , 'p_2_5_um_avg','p_5_0_um_avg', 'p_10_0_um_avg'\n",
    "                        ,'pm2_5_atm_avg','pm1_0_atm_avg', 'pm10_0_atm_avg'\n",
    "                        ,'pm1_pm25', 'pm25_pm10', 'pm1_pm10']]\n",
    "\n",
    "        Y_nd_val = df_nodust_val['tceq_pm25']\n",
    "\n",
    "    ############################### Train & Save Models #################################################\n",
    "\n",
    "        trained_model = model_formula.fit(VG7, Y)\n",
    "        trained_model_nd = model_formula.fit(VG7_nd, Y_nd)\n",
    "\n",
    "        # Save to file in the current working directory\n",
    "        pkl_filename = r\"C:\\Users\\{}\\OneDrive - UNT System\\AQ\\Calibration_Sensors\\Collocation_Data\\test_data\\trained_model.pkl\".format(computer)\n",
    "        with open(pkl_filename, 'wb') as file:\n",
    "            pickle.dump(trained_model, file)\n",
    "\n",
    "        # Save to file in the current working directory\n",
    "        pkl_filename_nd = r\"C:\\Users\\{}\\OneDrive - UNT System\\AQ\\Calibration_Sensors\\Collocation_Data\\test_data\\trained_model_nd.pkl\".format(computer)\n",
    "        with open(pkl_filename_nd, 'wb') as file:\n",
    "            pickle.dump(trained_model_nd, file)\n",
    "\n",
    "    ############################# Make Figure ###########################################\n",
    "        fig = plt.figure(figsize=(15,3))\n",
    "        ax = fig.add_subplot(132)\n",
    "        ax2 = fig.add_subplot(133)\n",
    "        ax3 = fig.add_subplot(131)\n",
    "        fig.suptitle('Split Model ' + model_type + ' Performance; Validation: ' + val, y=1.05, size=suptitle_size)\n",
    "\n",
    "        #Plot 131\n",
    "        ax3.scatter(df_dust_val['pm2_5_atm_avg'], df_dust_val['tceq_pm25'], label = str(df_dust_val.shape[0])+' dust hours', c=color)\n",
    "        ax3.scatter(df_nodust_val['pm2_5_atm_avg'], df_nodust_val['tceq_pm25'], label = str(df_nodust_val.shape[0])+' non-dust hours', alpha=0.6, c=color_nd)\n",
    "        ax3.set_ylabel('BAM PM2.5 (ug x m-3)', size=label_size)\n",
    "        ax3.set_xlabel('PurpleAir PM2.5 (ug x m-3)', size=label_size)\n",
    "        ax3.set_title(\"PA PM2.5 vs BAM BM2.5 Split by Filter\", size=title_size)#(str(model_type) + ' Split-Model: VG7')\n",
    "        ax3.legend(loc=2)     \n",
    "\n",
    "    ###################################### Dust #############################################\n",
    "        # Load from file\n",
    "        with open(pkl_filename, 'rb') as file:\n",
    "            pickle_model = pickle.load(file)\n",
    "\n",
    "        #Metrics\n",
    "        predicted = cross_val_predict(pickle_model, VG7_val, Y_val)\n",
    "        squared_error = (predicted - Y_val)**2\n",
    "        sum_squared_error = sum(squared_error)\n",
    "        predicted_mean = predicted.mean()\n",
    "        squared_total = (Y_val - predicted_mean)**2\n",
    "        sum_squared_total = sum(squared_total)\n",
    "        sum_squared_total\n",
    "        n = Y_val.shape[0]\n",
    "        k = VG7_val.shape[1]\n",
    "\n",
    "        #Score\n",
    "        rmse = np.sqrt(sum_squared_error/n)\n",
    "        RMSE = round(rmse,2)\n",
    "        Y_mean = Y_val.mean()\n",
    "        NRMSE = round(RMSE/Y_mean,2)\n",
    "        r2 = 1 - (sum_squared_error/sum_squared_total)\n",
    "        R2 = round(r2,2)\n",
    "        ar2 = 1-(((1-r2)*(n-1))/(n-k-1))\n",
    "        AR2 = round(ar2,2) \n",
    "        MAE = round(sum(abs(predicted - Y_val))/n,2)\n",
    "        nMAE = round((sum(abs(predicted - Y_val)))/n/Y_mean,2)\n",
    "\n",
    "\n",
    "        #Plot 132\n",
    "        ax.scatter(predicted, Y_val, c=color)\n",
    "        ax.plot([Y_val.min(), Y_val.max()], [Y_val.min(), Y_val.max()], 'k--', lw=4)\n",
    "        ax.set_xlabel('Predicted PM2.5', size=label_size)\n",
    "        ax.set_ylabel('Actual PM2.5', size=label_size)\n",
    "        ax.set_title(str(model_type) + ' Dust Model: VG7',size=title_size)\n",
    "        anchored_text = AnchoredText('MAE: ' + str(MAE) + '\\n'\n",
    "                                     'Adj. R2: ' + str(AR2), loc=2)\n",
    "        ax.add_artist(anchored_text)\n",
    "        anchored_text2 = AnchoredText('Filter: P10 > ' + str(round(p_size,2)) + \n",
    "          ' OR ' + '\\n' + 'PM2.5:PM10 < '+ str(round(threshold,2)), loc=4)\n",
    "        ax.add_artist(anchored_text2)\n",
    "\n",
    "    ##################################### No Dust ####################################\n",
    "        # Load from file\n",
    "        with open(pkl_filename_nd, 'rb') as file:\n",
    "            pickle_model_nd = pickle.load(file)\n",
    "\n",
    "\n",
    "        #Metrics\n",
    "        predicted_nd = cross_val_predict(pickle_model_nd, VG7_nd_val, Y_nd_val)\n",
    "        squared_error_nd = (predicted_nd - Y_nd_val)**2\n",
    "        sum_squared_error_nd = sum(squared_error_nd)\n",
    "        predicted_mean_nd = predicted_nd.mean()\n",
    "        squared_total_nd = (Y_nd_val - predicted_mean_nd)**2\n",
    "        sum_squared_total_nd = sum(squared_total_nd)\n",
    "        n_nd = Y_nd_val.shape[0]\n",
    "        k_nd = VG7_nd_val.shape[1]\n",
    "\n",
    "        #Score\n",
    "        rmse_nd = np.sqrt(sum_squared_error_nd/n_nd)\n",
    "        RMSE_nd = round(rmse_nd,2)\n",
    "        Y_mean_nd = Y_nd_val.mean()\n",
    "        NRMSE_nd = round(RMSE_nd/Y_mean_nd,2)\n",
    "        r2_nd = 1 - (sum_squared_error_nd/sum_squared_total_nd)\n",
    "        R2_nd = round(r2_nd,2)\n",
    "        ar2_nd = 1-(((1-r2_nd)*(n_nd-1))/(n_nd-k_nd-1))\n",
    "        AR2_nd = round(ar2_nd,2)\n",
    "        MAE_nd = round(sum(abs(predicted_nd - Y_nd_val))/n_nd,2)\n",
    "        nMAE_nd = round((sum(abs(predicted_nd - Y_nd_val)))/n_nd/Y_mean_nd,2)\n",
    "\n",
    "\n",
    "        #Plot 133\n",
    "        ax2.scatter(predicted_nd, Y_nd_val, c=color_nd)# edgecolors=(0, 0, 0))\n",
    "        ax2.plot([Y_nd_val.min(), Y_nd_val.max()], [Y_nd_val.min(), Y_nd_val.max()], 'k--', lw=4)\n",
    "        ax2.set_xlabel('Predicted PM2.5', size=label_size)\n",
    "        ax2.set_ylabel('Actual PM2.5', size = label_size)\n",
    "        ax2.set_title(str(model_type) + ' Non-Dust Model: VG7', size=title_size)\n",
    "        anchored_text = AnchoredText('MAE: ' + str(MAE_nd) + '\\n'\n",
    "                                     'Adj. R2: ' + str(AR2_nd), loc=2)\n",
    "        ax2.add_artist(anchored_text)\n",
    "        anchored_text2 = AnchoredText('Filter: P10 <= ' + str(round(p_size,2)) + \n",
    "                  ' OR ' + '\\n' + 'PM2.5:PM10 >= '+ str(round(threshold,2)), loc=4)\n",
    "        ax2.add_artist(anchored_text2)\n",
    "\n",
    "    ################################## Edit each time change is made ###########################################\n",
    "        particle_size_val.append(round(p_size,2))\n",
    "        keys_ratio_val.append(round(threshold,2))\n",
    "        dust_hours_ratio_val.append(df_dust.shape[0])\n",
    "        nodust_hours_ratio_val.append(df_nodust.shape[0])\n",
    "\n",
    "        dust_nrmse_ratio_val.append(NRMSE)\n",
    "        nodust_nrmse_ratio_val.append(NRMSE_nd)\n",
    "        nrmse_mean_ratio_val.append(round((NRMSE+NRMSE_nd)/2,2))\n",
    "\n",
    "        dust_rmse_ratio_val.append(RMSE)\n",
    "        nodust_rmse_ratio_val.append(RMSE_nd)\n",
    "        rmse_mean_ratio_val.append(round((RMSE+RMSE_nd)/2,2))\n",
    "        \n",
    "        dust_r2_val.append(R2)\n",
    "        nodust_r2_val.append(R2_nd)\n",
    "        r2_mean_val.append(round((R2+R2_nd)/2,2))\n",
    "        \n",
    "        dust_nmae_val.append(nMAE)\n",
    "        nodust_nmae_val.append(nMAE_nd)\n",
    "        mean_nmae_val.append(round((nMAE+nMAE_nd)/2,2))\n",
    "\n",
    "        dust_mae_val.append(MAE)\n",
    "        nodust_mae_val.append(MAE_nd)\n",
    "        mean_mae_val.append(round((MAE+MAE_nd)/2,2))\n",
    "\n",
    "    ################################## Edit each time change is made ###########################################\n",
    "print('Particle Size:', particle_size_val)\n",
    "print('Ratios:', keys_ratio_val)\n",
    "print('Dust Hours:', dust_hours_ratio_val)\n",
    "print('Non-Dust Hours:', nodust_hours_ratio_val)\n",
    "\n",
    "print('Dust NRMSE:', dust_nrmse_ratio_val)\n",
    "print('Non-Dust NRMSE:', nodust_nrmse_ratio_val)\n",
    "print('Mean NRMSE:', nrmse_mean_ratio_val)\n",
    "\n",
    "print('Dust RMSE:', dust_rmse_ratio_val)\n",
    "print('Non-Dust RMSE:', nodust_rmse_ratio_val)\n",
    "print('Mean RMSE:', rmse_mean_ratio_val)\n",
    "\n",
    "print('Dust R2:', dust_r2_val)\n",
    "print('Non-Dust R2:', nodust_r2_val)\n",
    "print('Mean R2:', nodust_r2_val)\n",
    "\n",
    "print('Dust NRMSE:', dust_mae_val)\n",
    "print('Non-Dust NRMSE:', nodust_mae_val)\n",
    "print('Mean NRMSE:', mean_mae_val)\n",
    "\n",
    "print('Dust NRMSE:', dust_nmae_val)\n",
    "print('Non-Dust NRMSE:', nodust_nmae_val)\n",
    "print('Mean NRMSE:', mean_nmae_val)\n",
    "    ################################## Edit each time change is made ###########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Model Performance as Influenced By PM2.5:PM10 Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfval = pd.DataFrame(\n",
    "    {'Particle Size': particle_size_val,\n",
    "    'Ratios':  keys_ratio_val,\n",
    "    'Dust Hours': dust_hours_ratio_val,\n",
    "    'Non-Dust Hours':nodust_hours_ratio_val,\n",
    "\n",
    "    'Dust NRMSE': dust_nrmse_ratio_val,\n",
    "    'Non-Dust NRMSE': nodust_nrmse_ratio_val,\n",
    "    'Mean NRMSE': nrmse_mean_ratio_val,\n",
    "\n",
    "    'Dust RMSE': dust_rmse_ratio_val,\n",
    "    'Non-Dust RMSE': nodust_rmse_ratio_val,\n",
    "    'Mean RMSE': rmse_mean_ratio_val,\n",
    "     \n",
    "    'Dust R2': dust_r2_val,\n",
    "    'Non-Dust R2': nodust_r2_val,\n",
    "    'Mean R2': r2_mean_val\n",
    "     \n",
    "    })\n",
    "\n",
    "df_meanrmse_val = dfval[['Ratios', 'Particle Size', 'Mean RMSE']]\n",
    "df_meanr2_val = dfval[['Ratios', 'Particle Size', 'Mean R2']]\n",
    "\n",
    "mean_array_val = df_meanrmse_val.to_numpy()\n",
    "meanr2_array_val = df_meanr2_val.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfval.sort_values(by=['Mean RMSE'], ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfval.sort_values(by=['Mean R2'], ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R2 Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "Xs = meanr2_array_val[:,0]\n",
    "Ys = meanr2_array_val[:,1]\n",
    "Zs = meanr2_array_val[:,2]\n",
    "\n",
    "\n",
    "# ======\n",
    "## plot:\n",
    "\n",
    "fig = plt.figure(figsize=(14,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "surf1 = ax.plot_trisurf(Xs, Ys, Zs, cmap=cm.Greens, linewidth=0)\n",
    "fig.colorbar(surf1, shrink=0.6, pad=0.01)\n",
    "\n",
    "\n",
    "#Dust Axes\n",
    "ax.xaxis.set_major_locator(MaxNLocator(6)) #Sets number of ticks on axis\n",
    "ax.set_xlabel('PM2.5:PM10 Ratio')\n",
    "ax.yaxis.set_major_locator(MaxNLocator(5))  #Sets number of ticks on axis\n",
    "ax.set_ylabel('Particle Size')\n",
    "ax.zaxis.set_major_locator(MaxNLocator(4))  #Sets number of ticks on axis\n",
    "ax.set_zlabel('RMSE', rotation=90)\n",
    "\n",
    "ax.text(0.96, 6, 6, \"Dust Models\", color='black', fontsize=12,fontweight=500)\n",
    "\n",
    "ax.view_init(5, 30) # controls orientation of 3D plot\n",
    "fig.tight_layout()\n",
    "fig.show() # or:\n",
    "# fig.savefig('3D.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "\n",
    "Xs = mean_array_val[:,0]\n",
    "Ys = mean_array_val[:,1]\n",
    "Zs = mean_array_val[:,2]\n",
    "\n",
    "\n",
    "# ======\n",
    "## plot:\n",
    "\n",
    "fig = plt.figure(figsize=(14,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "surf1 = ax.plot_trisurf(Xs, Ys, Zs, cmap=cm.Greens_r, linewidth=0)\n",
    "fig.colorbar(surf1, shrink=0.6, pad=0.01)\n",
    "\n",
    "\n",
    "#Dust Axes\n",
    "ax.xaxis.set_major_locator(MaxNLocator(6)) #Sets number of ticks on axis\n",
    "ax.set_xlabel('PM2.5:PM10 Ratio')\n",
    "ax.yaxis.set_major_locator(MaxNLocator(5))  #Sets number of ticks on axis\n",
    "ax.set_ylabel('Particle Size')\n",
    "ax.zaxis.set_major_locator(MaxNLocator(4))  #Sets number of ticks on axis\n",
    "ax.set_zlabel('RMSE', rotation=90)\n",
    "\n",
    "ax.text(0.96, 6, 6, \"Dust Models\", color='black', fontsize=12,fontweight=500)\n",
    "\n",
    "\n",
    "ax.view_init(5, 30) # controls orientation of 3D plot\n",
    "fig.tight_layout()\n",
    "fig.show() # or:\n",
    "# fig.savefig('3D.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test (Set 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "################################## Edit each time change is made #####################################\n",
    "particle_size_test = []\n",
    "keys_ratio_test = []\n",
    "dust_nrmse_ratio_test = []\n",
    "nodust_nrmse_ratio_test = []\n",
    "nrmse_mean_ratio_test = []\n",
    "dust_rmse_ratio_test = []\n",
    "nodust_rmse_ratio_test = []\n",
    "rmse_mean_ratio_test = []\n",
    "dust_hours_ratio_test= []\n",
    "nodust_hours_ratio_test = []\n",
    "r2_mean_test = []\n",
    "dust_r2_test = []\n",
    "nodust_r2_test = []\n",
    "dust_mae_test = []\n",
    "nodust_mae_test = []\n",
    "mean_mae_test = []\n",
    "dust_nmae_test = []\n",
    "nodust_nmae_test = []\n",
    "mean_nmae_test = []\n",
    "\n",
    "################################## PA-PM2.5 Treshold ###########################################\n",
    "# pm25 = \n",
    "for p_size in np.arange(start_,end_,increment_):\n",
    "    for threshold in np.arange(start,end,increment):\n",
    "#################################### Dust Filters #############################################\n",
    "\n",
    "        df_dust = df[(df['p_10_0_um_avg'] > p_size) | (df['pm25_pm10'] < threshold)]\n",
    "        df_nodust = df[~((df['p_10_0_um_avg'] > p_size) | (df['pm25_pm10'] < threshold))]\n",
    "\n",
    "        df_dust_test = df_test[(df_test['p_10_0_um_avg'] > p_size) | (df_test['pm25_pm10'] < threshold)]\n",
    "        df_nodust_test = df_test[~((df_test['p_10_0_um_avg'] > p_size) | (df_test['pm25_pm10'] < threshold))]\n",
    "\n",
    "    ################################### Slice Datasets ##########################################################\n",
    "    #################################### Training Set ##############################################################\n",
    "        VG7 = df_dust[['current_temp_f', 'current_humidity'\n",
    "                       ,'p_0_3_um_avg', 'p_0_5_um_avg', 'p_1_0_um_avg'\n",
    "                       , 'p_2_5_um_avg','p_5_0_um_avg', 'p_10_0_um_avg'\n",
    "                        ,'pm2_5_atm_avg','pm1_0_atm_avg', 'pm10_0_atm_avg'\n",
    "                        ,'pm1_pm25', 'pm25_pm10', 'pm1_pm10']]\n",
    "\n",
    "        Y = df_dust['tceq_pm25']\n",
    "\n",
    "        VG7_nd = df_nodust[['current_temp_f', 'current_humidity'\n",
    "                       ,'p_0_3_um_avg', 'p_0_5_um_avg', 'p_1_0_um_avg'\n",
    "                       , 'p_2_5_um_avg','p_5_0_um_avg', 'p_10_0_um_avg'\n",
    "                        ,'pm2_5_atm_avg','pm1_0_atm_avg', 'pm10_0_atm_avg'\n",
    "                        ,'pm1_pm25', 'pm25_pm10', 'pm1_pm10']]\n",
    "\n",
    "        Y_nd = df_nodust['tceq_pm25']\n",
    "\n",
    "    #################################### Validation Set ##############################################################\n",
    "        VG7_test = df_dust_test[['current_temp_f', 'current_humidity'\n",
    "                       ,'p_0_3_um_avg', 'p_0_5_um_avg', 'p_1_0_um_avg'\n",
    "                       , 'p_2_5_um_avg','p_5_0_um_avg', 'p_10_0_um_avg'\n",
    "                        ,'pm2_5_atm_avg','pm1_0_atm_avg', 'pm10_0_atm_avg'\n",
    "                        ,'pm1_pm25', 'pm25_pm10', 'pm1_pm10']]\n",
    "\n",
    "        Y_test = df_dust_test['tceq_pm25']\n",
    "\n",
    "        VG7_nd_test = df_nodust_test[['current_temp_f', 'current_humidity'\n",
    "                       ,'p_0_3_um_avg', 'p_0_5_um_avg', 'p_1_0_um_avg'\n",
    "                       , 'p_2_5_um_avg','p_5_0_um_avg', 'p_10_0_um_avg'\n",
    "                        ,'pm2_5_atm_avg','pm1_0_atm_avg', 'pm10_0_atm_avg'\n",
    "                        ,'pm1_pm25', 'pm25_pm10', 'pm1_pm10']]\n",
    "\n",
    "        Y_nd_test = df_nodust_test['tceq_pm25']\n",
    "\n",
    "    ############################### Train & Save Models #################################################\n",
    "        trained_model = model_formula.fit(VG7, Y)\n",
    "        # Save to file in the current working directory\n",
    "        pkl_filename = r\"C:\\Users\\{}\\OneDrive - UNT System\\AQ\\Calibration_Sensors\\Collocation_Data\\test_data\\trained_model.pkl\".format(computer)\n",
    "        with open(pkl_filename, 'wb') as file:\n",
    "            pickle.dump(trained_model, file)\n",
    "\n",
    "        trained_model_nd = model_formula.fit(VG7_nd, Y_nd)\n",
    "        # Save to file in the current working directory\n",
    "        pkl_filename_nd = r\"C:\\Users\\{}\\OneDrive - UNT System\\AQ\\Calibration_Sensors\\Collocation_Data\\test_data\\trained_model_nd.pkl\".format(computer)\n",
    "        with open(pkl_filename_nd, 'wb') as file:\n",
    "            pickle.dump(trained_model_nd, file)\n",
    "\n",
    "    ############################# Make Figure ###########################################\n",
    "        fig = plt.figure(figsize=(15,3))\n",
    "        ax = fig.add_subplot(132)\n",
    "        ax2 = fig.add_subplot(133)\n",
    "        ax3 = fig.add_subplot(131)\n",
    "        fig.suptitle('Split Model ' + model_type + ' Performance; Validation: ' + test, y=1.05, size=suptitle_size)\n",
    "\n",
    "        #Plot 131\n",
    "        ax3.scatter(df_dust_test['pm2_5_atm_avg'], df_dust_test['tceq_pm25'], label = str(df_dust_test.shape[0])+' dust hours', c=color)\n",
    "        ax3.scatter(df_nodust_test['pm2_5_atm_avg'], df_nodust_test['tceq_pm25'], label = str(df_nodust_test.shape[0])+' non-dust hours', alpha=0.6, c=color_nd)\n",
    "        ax3.set_ylabel('BAM PM2.5 (ug x m-3)', size=label_size)\n",
    "        ax3.set_xlabel('PurpleAir PM2.5 (ug x m-3)', size=label_size)\n",
    "        ax3.set_title(\"PA PM2.5 vs BAM BM2.5 Split by Filter\", size=title_size)#(str(model_type) + ' Split-Model: VG7')\n",
    "        ax3.legend(loc=2)    \n",
    "\n",
    "    ###################################### Dust #############################################\n",
    "        # Load from file\n",
    "        with open(pkl_filename, 'rb') as file:\n",
    "            pickle_model = pickle.load(file)\n",
    "\n",
    "        #Metrics\n",
    "        predicted = cross_val_predict(pickle_model, VG7_test, Y_test)\n",
    "        squared_error = (predicted - Y_test)**2\n",
    "        sum_squared_error = sum(squared_error)\n",
    "        predicted_mean = predicted.mean()\n",
    "        squared_total = (Y_test - predicted_mean)**2\n",
    "        sum_squared_total = sum(squared_total)\n",
    "        sum_squared_total\n",
    "        n = Y_test.shape[0]\n",
    "        k = VG7_test.shape[1]\n",
    "\n",
    "        #Score\n",
    "        rmse = np.sqrt(sum_squared_error/n)\n",
    "        RMSE = round(rmse,2)\n",
    "        Y_mean = Y_test.mean()\n",
    "        NRMSE = round(RMSE/Y_mean,2)\n",
    "        r2 = 1 - (sum_squared_error/sum_squared_total)\n",
    "        R2 = round(r2,2)\n",
    "        ar2 = 1-(((1-r2)*(n-1))/(n-k-1))\n",
    "        AR2 = round(ar2,2) \n",
    "        MAE = round(sum(abs(predicted - Y_test))/n,2)\n",
    "        nMAE = round((sum(abs(predicted - Y_test)))/n/Y_mean,2)\n",
    "\n",
    "        #Plot 132\n",
    "        ax.scatter(predicted, Y_test, c=color)\n",
    "        ax.plot([Y_test.min(), Y_test.max()], [Y_test.min(), Y_test.max()], 'k--', lw=4)\n",
    "        ax.set_xlabel('Predicted PM2.5', size=label_size)\n",
    "        ax.set_ylabel('Actual PM2.5', size=label_size)\n",
    "        ax.set_title(str(model_type) + ' Dust Model: VG7',size=title_size)\n",
    "        anchored_text = AnchoredText('MAE: ' + str(MAE) + '\\n'\n",
    "                                     'Adj. R2: ' + str(AR2), loc=2)\n",
    "        ax.add_artist(anchored_text)\n",
    "        anchored_text2 = AnchoredText('Filter: P10 > ' + str(round(p_size,2)) + \n",
    "          ' OR ' + '\\n' + 'PM2.5:PM10 < '+ str(round(threshold,2)), loc=4)\n",
    "        ax.add_artist(anchored_text2)\n",
    "    ##################################### No Dust ####################################\n",
    "        # Load from file\n",
    "        with open(pkl_filename_nd, 'rb') as file:\n",
    "            pickle_model_nd = pickle.load(file)\n",
    "\n",
    "\n",
    "        #Metrics\n",
    "        predicted_nd = cross_val_predict(pickle_model_nd, VG7_nd_test, Y_nd_test)\n",
    "        squared_error_nd = (predicted_nd - Y_nd_test)**2\n",
    "        sum_squared_error_nd = sum(squared_error_nd)\n",
    "        predicted_mean_nd = predicted_nd.mean()\n",
    "        squared_total_nd = (Y_nd_test - predicted_mean_nd)**2\n",
    "        sum_squared_total_nd = sum(squared_total_nd)\n",
    "        n_nd = Y_nd_test.shape[0]\n",
    "        k_nd = VG7_nd_test.shape[1]\n",
    "\n",
    "        #Score\n",
    "        rmse_nd = np.sqrt(sum_squared_error_nd/n_nd)\n",
    "        RMSE_nd = round(rmse_nd,2)\n",
    "        Y_mean_nd = Y_nd_test.mean()\n",
    "        NRMSE_nd = round(RMSE/Y_mean,2)\n",
    "        r2_nd = 1 - (sum_squared_error_nd/sum_squared_total_nd)\n",
    "        R2_nd = round(r2_nd,2)\n",
    "        ar2_nd = 1-(((1-r2_nd)*(n_nd-1))/(n_nd-k_nd-1))\n",
    "        AR2_nd = round(ar2_nd,2)\n",
    "        MAE_nd = round(sum(abs(predicted_nd - Y_nd_test))/n_nd,2)\n",
    "        nMAE_nd = round((sum(abs(predicted_nd - Y_nd_test)))/n_nd/Y_mean_nd,2)\n",
    "\n",
    "        #Plot 133\n",
    "        ax2.scatter(predicted_nd, Y_nd_test, c=color_nd)# edgecolors=(0, 0, 0))\n",
    "        ax2.plot([Y_nd_test.min(), Y_nd_test.max()], [Y_nd_test.min(), Y_nd_test.max()], 'k--', lw=4)\n",
    "        ax2.set_xlabel('Predicted PM2.5', size=label_size)\n",
    "        ax2.set_ylabel('Actual PM2.5', size = label_size)\n",
    "        ax2.set_title(str(model_type) + ' Non-Dust Model: VG7', size=title_size)\n",
    "        anchored_text = AnchoredText('MAE: ' + str(MAE_nd) + '\\n'\n",
    "                                     'Adj. R2: ' + str(AR2_nd), loc=2)\n",
    "        ax2.add_artist(anchored_text)\n",
    "        anchored_text2 = AnchoredText('Filter: P10 <= ' + str(round(p_size,2)) + \n",
    "                  ' OR ' + '\\n' + 'PM2.5:PM10 >= '+ str(round(threshold,2)), loc=4)\n",
    "        ax2.add_artist(anchored_text2)\n",
    "\n",
    "    ################################## Edit each time change is made ###########################################\n",
    "        particle_size_test.append(round(p_size,2))\n",
    "        keys_ratio_test.append(round(threshold,2))\n",
    "        dust_hours_ratio_test.append(df_dust.shape[0])\n",
    "        nodust_hours_ratio_test.append(df_nodust.shape[0])\n",
    "\n",
    "        dust_nrmse_ratio_test.append(NRMSE)\n",
    "        nodust_nrmse_ratio_test.append(NRMSE_nd)\n",
    "        nrmse_mean_ratio_test.append(round((NRMSE+NRMSE_nd)/2,2))\n",
    "\n",
    "        dust_rmse_ratio_test.append(RMSE)\n",
    "        nodust_rmse_ratio_test.append(RMSE_nd)\n",
    "        rmse_mean_ratio_test.append(round((RMSE+RMSE_nd)/2,2))\n",
    "        \n",
    "        dust_r2_test.append(R2)\n",
    "        nodust_r2_test.append(R2_nd)\n",
    "        r2_mean_test.append(round((R2+R2_nd)/2,2))\n",
    "        \n",
    "        dust_nmae_test.append(nMAE)\n",
    "        nodust_nmae_test.append(nMAE_nd)\n",
    "        mean_nmae_test.append(round((nMAE+nMAE_nd)/2,2))\n",
    "\n",
    "        dust_mae_test.append(MAE)\n",
    "        nodust_mae_test.append(MAE_nd)\n",
    "        mean_mae_test.append(round((MAE+MAE_nd)/2,2))\n",
    "\n",
    "    ################################## Edit each time change is made ###########################################\n",
    "print('Particle Size:', particle_size_test)\n",
    "print('Ratios:', keys_ratio_test)\n",
    "print('Dust Hours:', dust_hours_ratio_test)\n",
    "print('Non-Dust Hours:', nodust_hours_ratio_test)\n",
    "\n",
    "print('Dust NRMSE:', dust_nrmse_ratio_test)\n",
    "print('Non-Dust NRMSE:', nodust_nrmse_ratio_test)\n",
    "print('Mean NRMSE:', nrmse_mean_ratio_test)\n",
    "\n",
    "print('Dust RMSE:', dust_rmse_ratio_test)\n",
    "print('Non-Dust RMSE:', nodust_rmse_ratio_test)\n",
    "print('Mean RMSE:', rmse_mean_ratio_test)\n",
    "\n",
    "print('Dust R2:', dust_r2_test)\n",
    "print('Non-Dust R2:', nodust_r2_test)\n",
    "print('Mean R2:', r2_mean_test)\n",
    "\n",
    "print('Dust NRMSE:', dust_mae_test)\n",
    "print('Non-Dust NRMSE:', nodust_mae_test)\n",
    "print('Mean NRMSE:', mean_mae_test)\n",
    "\n",
    "print('Dust NRMSE:', dust_nmae_test)\n",
    "print('Non-Dust NRMSE:', nodust_nmae_test)\n",
    "print('Mean NRMSE:', mean_nmae_test)\n",
    "    ################################## Edit each time change is made ###########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Model Performance as Influenced By PM2.5:PM10 Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest = pd.DataFrame(\n",
    "    {'Particle Size': particle_size_test,\n",
    "    'Ratios':  keys_ratio_test,\n",
    "    'Dust Hours': dust_hours_ratio_test,\n",
    "    'Non-Dust Hours':nodust_hours_ratio_test,\n",
    "\n",
    "    'Dust NRMSE': dust_nrmse_ratio_test,\n",
    "    'Non-Dust NRMSE': nodust_nrmse_ratio_test,\n",
    "    'Mean NRMSE': nrmse_mean_ratio_test,\n",
    "\n",
    "    'Dust RMSE': dust_rmse_ratio_test,\n",
    "    'Non-Dust RMSE': nodust_rmse_ratio_test,\n",
    "    'Mean RMSE': rmse_mean_ratio_test,\n",
    "     \n",
    "    'Dust R2': dust_r2_test,\n",
    "    'Non-Dust R2': nodust_r2_test,\n",
    "    'Mean R2': r2_mean_test\n",
    "     \n",
    "    })\n",
    "\n",
    "df_meanrmse_test = dftest[['Ratios', 'Particle Size', 'Mean RMSE']]\n",
    "df_meanr2_test = dftest[['Ratios', 'Particle Size', 'Mean R2']]\n",
    "\n",
    "\n",
    "mean_array_test = df_meanrmse_test.to_numpy()\n",
    "meanr2_array_test = df_meanr2_test.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dftest_Filter = dftest[dftest['Non-Dust Hours']>1200]\n",
    "dftest.sort_values(by=['Mean RMSE'], ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest_Filter.sort_values(by=['Mean R2'], ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R2 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib \n",
    "# %matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "\n",
    "Xs = meanr2_array_test[:,0]\n",
    "Ys = meanr2_array_test[:,1]\n",
    "Zs = meanr2_array_test[:,2]\n",
    "\n",
    "\n",
    "# ======\n",
    "## plot:\n",
    "\n",
    "fig = plt.figure(figsize=(14,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "surf1 = ax.plot_trisurf(Xs, Ys, Zs, cmap=cm.Greens, linewidth=0)\n",
    "fig.colorbar(surf1, shrink=0.6, pad=0.01)\n",
    "\n",
    "\n",
    "#Dust Axes\n",
    "ax.xaxis.set_major_locator(MaxNLocator(6)) #Sets number of ticks on axis\n",
    "ax.set_xlabel('PM2.5:PM10 Ratio')\n",
    "ax.yaxis.set_major_locator(MaxNLocator(8))  #Sets number of ticks on axis\n",
    "ax.set_ylabel('Particle Size')\n",
    "ax.zaxis.set_major_locator(MaxNLocator(4))  #Sets number of ticks on axis\n",
    "ax.set_zlabel('RMSE', rotation=90)\n",
    "\n",
    "ax.text(0.96, 6, 6, \"Dust Models\", color='black', fontsize=12,fontweight=500)\n",
    "\n",
    "ax.view_init(5, 30) # controls orientation of 3D plot\n",
    "fig.tight_layout()\n",
    "fig.show() # or:\n",
    "# fig.savefig('3D.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "\n",
    "Xs = mean_array_test[:,0]\n",
    "Ys = mean_array_test[:,1]\n",
    "Zs = mean_array_test[:,2]\n",
    "\n",
    "\n",
    "# ======\n",
    "## plot:\n",
    "\n",
    "fig = plt.figure(figsize=(14,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "surf1 = ax.plot_trisurf(Xs, Ys, Zs, cmap=cm.Greens_r, linewidth=0)\n",
    "fig.colorbar(surf1, shrink=0.6, pad=0.01)\n",
    "\n",
    "\n",
    "#Dust Axes\n",
    "ax.xaxis.set_major_locator(MaxNLocator(6)) #Sets number of ticks on axis\n",
    "ax.set_xlabel('PM2.5:PM10 Ratio')\n",
    "ax.yaxis.set_major_locator(MaxNLocator(5))  #Sets number of ticks on axis\n",
    "ax.set_ylabel('Particle Size')\n",
    "ax.zaxis.set_major_locator(MaxNLocator(4))  #Sets number of ticks on axis\n",
    "ax.set_zlabel('RMSE', rotation=90)\n",
    "\n",
    "ax.text(0.96, 6, 6, \"Dust Models\", color='black', fontsize=12,fontweight=500)\n",
    "\n",
    "\n",
    "ax.view_init(5, 30) # controls orientation of 3D plot\n",
    "fig.tight_layout()\n",
    "fig.show() # or:\n",
    "# fig.savefig('3D.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2 (Set 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "################################## Edit each time change is made #####################################\n",
    "particle_size_test2 = []\n",
    "keys_ratio_test2 = []\n",
    "dust_nrmse_ratio_test2 = []\n",
    "nodust_nrmse_ratio_test2 = []\n",
    "nrmse_mean_ratio_test2 = []\n",
    "dust_rmse_ratio_test2 = []\n",
    "nodust_rmse_ratio_test2 = []\n",
    "rmse_mean_ratio_test2 = []\n",
    "dust_hours_ratio_test2 = []\n",
    "nodust_hours_ratio_test2 = []\n",
    "r2_mean_test2 = []\n",
    "dust_r2_test2 = []\n",
    "nodust_r2_test2 = []\n",
    "dust_mae_test2 = []\n",
    "nodust_mae_test2 = []\n",
    "mean_mae_test2 = []\n",
    "dust_nmae_test2 = []\n",
    "nodust_nmae_test2 = []\n",
    "mean_nmae_test2 = []\n",
    "\n",
    "################################## PA-PM2.5 Treshold ###########################################\n",
    "# pm25 = \n",
    "for p_size in np.arange(start_,end_,increment_):\n",
    "    for threshold in np.arange(start,end,increment):\n",
    "#################################### Dust Filters #############################################\n",
    "\n",
    "        df_dust = df[(df['p_10_0_um_avg'] > p_size) | (df['pm25_pm10'] < threshold)]\n",
    "        df_nodust = df[~((df['p_10_0_um_avg'] > p_size) | (df['pm25_pm10'] < threshold))]\n",
    "\n",
    "        df_dust_test2 = df_test2[(df_test2['p_10_0_um_avg'] > p_size) | (df_test2['pm25_pm10'] < threshold)]\n",
    "        df_nodust_test2 = df_test2[~((df_test2['p_10_0_um_avg'] > p_size) | (df_test2['pm25_pm10'] < threshold))]\n",
    "\n",
    "    ################################### Slice Datasets ##########################################################\n",
    "    #################################### Training Set ##############################################################\n",
    "        VG7 = df_dust[['current_temp_f', 'current_humidity'\n",
    "                       ,'p_0_3_um_avg', 'p_0_5_um_avg', 'p_1_0_um_avg'\n",
    "                       , 'p_2_5_um_avg','p_5_0_um_avg', 'p_10_0_um_avg'\n",
    "                        ,'pm2_5_atm_avg','pm1_0_atm_avg', 'pm10_0_atm_avg'\n",
    "                        ,'pm1_pm25', 'pm25_pm10', 'pm1_pm10']]\n",
    "\n",
    "        Y = df_dust['tceq_pm25']\n",
    "\n",
    "        VG7_nd = df_nodust[['current_temp_f', 'current_humidity'\n",
    "                       ,'p_0_3_um_avg', 'p_0_5_um_avg', 'p_1_0_um_avg'\n",
    "                       , 'p_2_5_um_avg','p_5_0_um_avg', 'p_10_0_um_avg'\n",
    "                        ,'pm2_5_atm_avg','pm1_0_atm_avg', 'pm10_0_atm_avg'\n",
    "                        ,'pm1_pm25', 'pm25_pm10', 'pm1_pm10']]\n",
    "\n",
    "        Y_nd = df_nodust['tceq_pm25']\n",
    "\n",
    "    #################################### Validation Set ##############################################################\n",
    "        VG7_test2 = df_dust_test2[['current_temp_f', 'current_humidity'\n",
    "                       ,'p_0_3_um_avg', 'p_0_5_um_avg', 'p_1_0_um_avg'\n",
    "                       , 'p_2_5_um_avg','p_5_0_um_avg', 'p_10_0_um_avg'\n",
    "                        ,'pm2_5_atm_avg','pm1_0_atm_avg', 'pm10_0_atm_avg'\n",
    "                        ,'pm1_pm25', 'pm25_pm10', 'pm1_pm10']]\n",
    "\n",
    "        Y_test2 = df_dust_test2['tceq_pm25']\n",
    "\n",
    "        VG7_nd_test2 = df_nodust_test2[['current_temp_f', 'current_humidity'\n",
    "                       ,'p_0_3_um_avg', 'p_0_5_um_avg', 'p_1_0_um_avg'\n",
    "                       , 'p_2_5_um_avg','p_5_0_um_avg', 'p_10_0_um_avg'\n",
    "                        ,'pm2_5_atm_avg','pm1_0_atm_avg', 'pm10_0_atm_avg'\n",
    "                        ,'pm1_pm25', 'pm25_pm10', 'pm1_pm10']]\n",
    "\n",
    "        Y_nd_test2 = df_nodust_test2['tceq_pm25']\n",
    "\n",
    "    ############################### Train & Save Models #################################################\n",
    "        trained_model = model_formula.fit(VG7, Y)\n",
    "        # Save to file in the current working directory\n",
    "        pkl_filename = r\"C:\\Users\\{}\\OneDrive - UNT System\\AQ\\Calibration_Sensors\\Collocation_Data\\test_data\\trained_model.pkl\".format(computer)\n",
    "        with open(pkl_filename, 'wb') as file:\n",
    "            pickle.dump(trained_model, file)\n",
    "\n",
    "        trained_model_nd = model_formula.fit(VG7_nd, Y_nd)\n",
    "        # Save to file in the current working directory\n",
    "        pkl_filename_nd = r\"C:\\Users\\{}\\OneDrive - UNT System\\AQ\\Calibration_Sensors\\Collocation_Data\\test_data\\trained_model_nd.pkl\".format(computer)\n",
    "        with open(pkl_filename_nd, 'wb') as file:\n",
    "            pickle.dump(trained_model_nd, file)\n",
    "\n",
    "    ############################# Make Figure ###########################################\n",
    "        fig = plt.figure(figsize=(15,3))\n",
    "        ax = fig.add_subplot(132)\n",
    "        ax2 = fig.add_subplot(133)\n",
    "        ax3 = fig.add_subplot(131)\n",
    "        fig.suptitle('Split Model ' + model_type + ' Performance; Validation: ' + test2, y=1.05, size=suptitle_size)\n",
    "\n",
    "        #Plot 131\n",
    "        ax3.scatter(df_dust_test2['pm2_5_atm_avg'], df_dust_test2['tceq_pm25'], label = str(df_dust_test2.shape[0])+' dust hours', c=color)\n",
    "        ax3.scatter(df_nodust_test2['pm2_5_atm_avg'], df_nodust_test2['tceq_pm25'], label = str(df_nodust_test2.shape[0])+' non-dust hours', alpha=0.6, c=color_nd)\n",
    "        ax3.set_ylabel('BAM PM2.5 (ug x m-3)', size=label_size)\n",
    "        ax3.set_xlabel('PurpleAir PM2.5 (ug x m-3)', size=label_size)\n",
    "        ax3.set_title(\"PA PM2.5 vs BAM BM2.5 Split by Filter\", size=title_size)#(str(model_type) + ' Split-Model: VG7')\n",
    "        ax3.legend(loc=2)   \n",
    "\n",
    "    ###################################### Dust #############################################\n",
    "        # Load from file\n",
    "        with open(pkl_filename, 'rb') as file:\n",
    "            pickle_model = pickle.load(file)\n",
    "\n",
    "        #Metrics\n",
    "        predicted = cross_val_predict(pickle_model, VG7_test2, Y_test2)\n",
    "        squared_error = (predicted - Y_test2)**2\n",
    "        sum_squared_error = sum(squared_error)\n",
    "        predicted_mean = predicted.mean()\n",
    "        squared_total = (Y_test2 - predicted_mean)**2\n",
    "        sum_squared_total = sum(squared_total)\n",
    "        sum_squared_total\n",
    "        n = Y_test2.shape[0]\n",
    "        k = VG7_test2.shape[1]\n",
    "\n",
    "        #Score\n",
    "        rmse = np.sqrt(sum_squared_error/n)\n",
    "        RMSE = round(rmse,2)\n",
    "        Y_mean = Y_test2.mean()\n",
    "        NRMSE = round(RMSE/Y_mean,2)\n",
    "        r2 = 1 - (sum_squared_error/sum_squared_total)\n",
    "        R2 = round(r2,2)\n",
    "        ar2 = 1-(((1-r2)*(n-1))/(n-k-1))\n",
    "        AR2 = round(ar2,2) \n",
    "        MAE = round(sum(abs(predicted - Y_test2))/n,2)\n",
    "        nMAE = round((sum(abs(predicted - Y_test2)))/n/Y_mean,2)\n",
    "\n",
    "        #Plot 132\n",
    "        ax.scatter(predicted, Y_test2, c=color)\n",
    "        ax.plot([Y_test2.min(), Y_test2.max()], [Y_test2.min(), Y_test2.max()], 'k--', lw=4)\n",
    "        ax.set_xlabel('Predicted PM2.5', size=label_size)\n",
    "        ax.set_ylabel('Actual PM2.5', size=label_size)\n",
    "        ax.set_title(str(model_type) + ' Dust Model: VG7',size=title_size)\n",
    "        anchored_text = AnchoredText('MAE: ' + str(MAE) + '\\n'\n",
    "                                     'Adj. R2: ' + str(AR2), loc=2)\n",
    "        ax.add_artist(anchored_text)\n",
    "        anchored_text2 = AnchoredText('Filter: P10 > ' + str(round(p_size,2)) + \n",
    "          ' OR ' + '\\n' + 'PM2.5:PM10 < '+ str(round(threshold,2)), loc=4)\n",
    "        ax.add_artist(anchored_text2)\n",
    "\n",
    "    ##################################### No Dust ####################################\n",
    "        # Load from file\n",
    "        with open(pkl_filename_nd, 'rb') as file:\n",
    "            pickle_model_nd = pickle.load(file)\n",
    "\n",
    "\n",
    "        #Metrics\n",
    "        predicted_nd = cross_val_predict(pickle_model_nd, VG7_nd_test2, Y_nd_test2)\n",
    "        squared_error_nd = (predicted_nd - Y_nd_test2)**2\n",
    "        sum_squared_error_nd = sum(squared_error_nd)\n",
    "        predicted_mean_nd = predicted_nd.mean()\n",
    "        squared_total_nd = (Y_nd_test2 - predicted_mean_nd)**2\n",
    "        sum_squared_total_nd = sum(squared_total_nd)\n",
    "        n_nd = Y_nd_test2.shape[0]\n",
    "        k_nd = VG7_nd_test2.shape[1]\n",
    "\n",
    "        #Score\n",
    "        rmse_nd = np.sqrt(sum_squared_error_nd/n_nd)\n",
    "        RMSE_nd = round(rmse_nd,2)\n",
    "        Y_mean_nd = Y_nd_test2.mean()\n",
    "        NRMSE_nd = round(RMSE/Y_mean,2)\n",
    "        r2_nd = 1 - (sum_squared_error_nd/sum_squared_total_nd)\n",
    "        R2_nd = round(r2_nd,2)\n",
    "        ar2_nd = 1-(((1-r2_nd)*(n_nd-1))/(n_nd-k_nd-1))\n",
    "        AR2_nd = round(ar2_nd,2)\n",
    "        MAE_nd = round(sum(abs(predicted_nd - Y_nd_test2))/n_nd,2)\n",
    "        nMAE_nd = round((sum(abs(predicted_nd - Y_nd_test2)))/n_nd/Y_mean_nd,2)\n",
    "\n",
    "        #Plot 133\n",
    "        ax2.scatter(predicted_nd, Y_nd_test2, c=color_nd)# edgecolors=(0, 0, 0))\n",
    "        ax2.plot([Y_nd_test2.min(), Y_nd_test2.max()], [Y_nd_test2.min(), Y_nd_test2.max()], 'k--', lw=4)\n",
    "        ax2.set_xlabel('Predicted PM2.5', size=label_size)\n",
    "        ax2.set_ylabel('Actual PM2.5', size = label_size)\n",
    "        ax2.set_title(str(model_type) + ' Non-Dust Model: VG7', size=title_size)\n",
    "        anchored_text = AnchoredText('MAE: ' + str(MAE_nd) + '\\n'\n",
    "                                     'Adj. R2: ' + str(AR2_nd), loc=2)\n",
    "        ax2.add_artist(anchored_text)\n",
    "        anchored_text2 = AnchoredText('Filter: P10 <= ' + str(round(p_size,2)) + \n",
    "                  ' OR ' + '\\n' + 'PM2.5:PM10 >= '+ str(round(threshold,2)), loc=4)\n",
    "        ax2.add_artist(anchored_text2)\n",
    "\n",
    "    ################################## Edit each time change is made ###########################################\n",
    "        particle_size_test2.append(round(p_size,2))\n",
    "        keys_ratio_test2.append(round(threshold,2))\n",
    "        dust_hours_ratio_test2.append(df_dust.shape[0])\n",
    "        nodust_hours_ratio_test2.append(df_nodust.shape[0])\n",
    "\n",
    "        dust_nrmse_ratio_test2.append(NRMSE)\n",
    "        nodust_nrmse_ratio_test2.append(NRMSE_nd)\n",
    "        nrmse_mean_ratio_test2.append(round((NRMSE+NRMSE_nd)/2,2))\n",
    "\n",
    "        dust_rmse_ratio_test2.append(RMSE)\n",
    "        nodust_rmse_ratio_test2.append(RMSE_nd)\n",
    "        rmse_mean_ratio_test2.append(round((RMSE+RMSE_nd)/2,2))\n",
    "        \n",
    "        dust_r2_test2.append(R2)\n",
    "        nodust_r2_test2.append(R2_nd)\n",
    "        r2_mean_test2.append(round((R2+R2_nd)/2,2))\n",
    "        \n",
    "        dust_nmae_test2.append(nMAE)\n",
    "        nodust_nmae_test2.append(nMAE_nd)\n",
    "        mean_nmae_test2.append(round((nMAE+nMAE_nd)/2,2))\n",
    "\n",
    "        dust_mae_test2.append(MAE)\n",
    "        nodust_mae_test2.append(MAE_nd)\n",
    "        mean_mae_test2.append(round((MAE+MAE_nd)/2,2))\n",
    "\n",
    "    ################################## Edit each time change is made ###########################################\n",
    "print('Particle Size:', particle_size_test2)\n",
    "print('Ratios:', keys_ratio_test2)\n",
    "print('Dust Hours:', dust_hours_ratio_test2)\n",
    "print('Non-Dust Hours:', nodust_hours_ratio_test2)\n",
    "\n",
    "print('Dust NRMSE:', dust_nrmse_ratio_test2)\n",
    "print('Non-Dust NRMSE:', nodust_nrmse_ratio_test2)\n",
    "print('Mean NRMSE:', nrmse_mean_ratio_test2)\n",
    "\n",
    "print('Dust RMSE:', dust_rmse_ratio_test2)\n",
    "print('Non-Dust RMSE:', nodust_rmse_ratio_test2)\n",
    "print('Mean RMSE:', rmse_mean_ratio_test2)\n",
    "\n",
    "print('Dust R2:', dust_r2_test2)\n",
    "print('Non-Dust R2:', nodust_r2_test2)\n",
    "print('Mean R2:', r2_mean_test2)\n",
    "\n",
    "print('Dust NRMSE:', dust_mae_test2)\n",
    "print('Non-Dust NRMSE:', nodust_mae_test2)\n",
    "print('Mean NRMSE:', mean_mae_test2)\n",
    "\n",
    "print('Dust NRMSE:', dust_nmae_test2)\n",
    "print('Non-Dust NRMSE:', nodust_nmae_test2)\n",
    "print('Mean NRMSE:', mean_nmae_test2)\n",
    "    ################################## Edit each time change is made ###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_nodust.to_csv(r'C:\\Users\\sph0088\\OneDrive - UNT System\\AQ\\Calibration_Sensors\\Collocation_Data\\test_data\\outlier2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest2 = pd.DataFrame(\n",
    "    {'Particle Size': particle_size_test2,\n",
    "    'Ratios':  keys_ratio_test2,\n",
    "    'Dust Hours': dust_hours_ratio_test2,\n",
    "    'Non-Dust Hours':nodust_hours_ratio_test2,\n",
    "\n",
    "    'Dust NRMSE': dust_nrmse_ratio_test2,\n",
    "    'Non-Dust NRMSE': nodust_nrmse_ratio_test2,\n",
    "    'Mean NRMSE': nrmse_mean_ratio_test2,\n",
    "\n",
    "    'Dust RMSE': dust_rmse_ratio_test2,\n",
    "    'Non-Dust RMSE': nodust_rmse_ratio_test2,\n",
    "    'Mean RMSE': rmse_mean_ratio_test2,\n",
    "     \n",
    "    'Dust R2': dust_r2_test2,\n",
    "    'Non-Dust R2': nodust_r2_test2,\n",
    "    'Mean R2': r2_mean_test2\n",
    "     \n",
    "    })\n",
    "\n",
    "df_meanrmse_test2 = dftest2[['Ratios', 'Particle Size', 'Mean RMSE']]\n",
    "df_meanr2_test2 = dftest2[['Ratios', 'Particle Size', 'Mean R2']]\n",
    "\n",
    "\n",
    "mean_array_test2 = df_meanrmse_test2.to_numpy()\n",
    "meanr2_array_test2 = df_meanr2_test2.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest2_Filter = dftest2[dftest2['Non-Dust Hours']>1200]\n",
    "dftest2_Filter.sort_values(by=['Mean RMSE'], ascending=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest2_Filter.sort_values(by=['Mean R2'], ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R2 Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "%matplotlib \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "\n",
    "Xs = meanr2_array_test2[:,0]\n",
    "Ys = meanr2_array_test2[:,1]\n",
    "Zs = meanr2_array_test2[:,2]\n",
    "\n",
    "\n",
    "# ======\n",
    "## plot:\n",
    "\n",
    "fig = plt.figure(figsize=(14,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "surf1 = ax.plot_trisurf(Xs, Ys, Zs, cmap=cm.Greens, linewidth=0)\n",
    "fig.colorbar(surf1, shrink=0.6, pad=0.01)\n",
    "\n",
    "\n",
    "#Dust Axes\n",
    "ax.xaxis.set_major_locator(MaxNLocator(6)) #Sets number of ticks on axis\n",
    "ax.set_xlabel('PM2.5:PM10 Ratio')\n",
    "ax.yaxis.set_major_locator(MaxNLocator(8))  #Sets number of ticks on axis\n",
    "ax.set_ylabel('Particle Size')\n",
    "ax.zaxis.set_major_locator(MaxNLocator(4))  #Sets number of ticks on axis\n",
    "ax.set_zlabel('RMSE', rotation=90)\n",
    "\n",
    "ax.text(0.96, 6, 6, \"Dust Models\", color='black', fontsize=12,fontweight=500)\n",
    "\n",
    "\n",
    "ax.view_init(5, 30) # controls orientation of 3D plot\n",
    "fig.tight_layout()\n",
    "fig.show() # or:\n",
    "# fig.savefig('3D.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSE Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib \n",
    "# %matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "\n",
    "Xs = mean_array_test2[:,0]\n",
    "Ys = mean_array_test2[:,1]\n",
    "Zs = mean_array_test2[:,2]\n",
    "\n",
    "# ======\n",
    "## plot:\n",
    "\n",
    "fig = plt.figure(figsize=(14,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "surf1 = ax.plot_trisurf(Xs, Ys, Zs, cmap=cm.Greens_r, linewidth=0)\n",
    "fig.colorbar(surf1, shrink=0.6, pad=0.01)\n",
    "\n",
    "\n",
    "#Dust Axes\n",
    "ax.xaxis.set_major_locator(MaxNLocator(6)) #Sets number of ticks on axis\n",
    "ax.set_xlabel('PM2.5:PM10 Ratio')\n",
    "ax.yaxis.set_major_locator(MaxNLocator(5))  #Sets number of ticks on axis\n",
    "ax.set_ylabel('Particle Size')\n",
    "ax.zaxis.set_major_locator(MaxNLocator(4))  #Sets number of ticks on axis\n",
    "ax.set_zlabel('RMSE', rotation=90)\n",
    "\n",
    "ax.text(0.96, 6, 6, \"Dust Models\", color='black', fontsize=12,fontweight=500)\n",
    "\n",
    "\n",
    "ax.view_init(5, 30) # controls orientation of 3D plot\n",
    "fig.tight_layout()\n",
    "fig.show() # or:\n",
    "# fig.savefig('3D.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Averages Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrmse_sets = np.array([nrmse_mean_ratio_val, nrmse_mean_ratio_test, nrmse_mean_ratio_test2])\n",
    "nrmse_mean_sets = np.average(nrmse_sets, axis=0)\n",
    "\n",
    "rmse_sets = np.array([rmse_mean_ratio_val, rmse_mean_ratio_test, rmse_mean_ratio_test2])\n",
    "rmse_mean_sets = np.average(rmse_sets, axis=0)\n",
    "\n",
    "r2_sets = np.array([r2_mean_val, r2_mean_test, r2_mean_test2])\n",
    "r2_mean_sets = np.average(r2_sets, axis=0)\n",
    "\n",
    "nmae_sets = np.array([mean_nmae_val, mean_nmae_test, mean_nmae_test2])\n",
    "nmae_mean_sets = np.average(nmae_sets, axis=0)\n",
    "\n",
    "mae_sets = np.array([mean_mae_val, mean_mae_test, mean_mae_test2])\n",
    "mae_mean_sets = np.average(mae_sets, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "cmap = LinearSegmentedColormap.from_list('name', ['red', 'blue', 'orange', 'yellow', 'green', 'purple', 'black'])\n",
    "#### Plot ######\n",
    "fig = plt.figure(figsize=(16,6))\n",
    "ax = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "color = ax.scatter(keys_ratio_test[:], nmae_mean_sets[:], marker='o',label = 'Mean of 2 sets',\n",
    "                   c=particle_size_val, cmap = cmap)\n",
    "ax.set_xticks(np.arange(min(keys_ratio_test), max(keys_ratio_test)+0.01, 0.04))\n",
    "ax.set_xlabel('PM2.5:PM10 Threshold')\n",
    "ax.set_ylabel('Mean NMAE Between Three Models')\n",
    "cbar = fig.colorbar(color, ticks=[0.3, 0.5, 0.7, 0.9, 1.1, 1.3, 1.5])\n",
    "\n",
    "ax2.scatter(particle_size_val[:], nmae_mean_sets[:], marker='o',label = 'Mean of 2 sets')\n",
    "ax2.set_xlabel('P10 Threshold')\n",
    "ax2.set_ylabel('Mean NMAE Between Three Models')\n",
    "\n",
    "plt.suptitle('Mean NMAE of Dust and Non-Dust ' + model_type + ' Models Across Thresholds --' +' Training: ' + training + '; Validation: ' + val +', '+ test + ' and ' + test2, y=0.92)\n",
    "# plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "    \n",
    "cmap = LinearSegmentedColormap.from_list('name', ['red', 'blue', 'orange', 'yellow', 'green', 'purple', 'black'])\n",
    "\n",
    "#### Plot ######\n",
    "fig = plt.figure(figsize=(16,6))\n",
    "ax = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "color = ax.scatter(keys_ratio_test[:], mae_mean_sets[:], marker='o',label = 'Mean of 2 sets',\n",
    "                   c=particle_size_val, cmap = cmap)\n",
    "ax.set_xticks(np.arange(min(keys_ratio_test), max(keys_ratio_test)+0.01, 0.04))\n",
    "ax.set_xlabel('PM2.5:PM10 Threshold')\n",
    "ax.set_ylabel('Mean MAE Between Three Models')\n",
    "cbar = fig.colorbar(color, ticks=[0.3, 0.5, 0.7, 0.9, 1.1, 1.3, 1.5])\n",
    "\n",
    "\n",
    "ax2.scatter(particle_size_val[:], mae_mean_sets[:], marker='o',label = 'Mean of 2 sets')\n",
    "# ax2.set_xticks(np.arange(min(x), max(x)+0.6, 0.2))\n",
    "ax2.set_xlabel('P10 Threshold')\n",
    "ax2.set_ylabel('Mean MAE Between Three Models')\n",
    "\n",
    "plt.suptitle('Mean MAE of Dust and Non-Dust ' + model_type + ' Models Across Thresholds --' +' Training: ' + training + '; Validation: ' + val +', '+ test + ' and ' + test2, y=0.92)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "    \n",
    "#### Plot ######\n",
    "fig = plt.figure(figsize=(16,6))\n",
    "ax = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "color = ax.scatter(keys_ratio_test[:], nrmse_mean_sets[:], marker='o',label = 'Mean of 2 sets',\n",
    "                  c = particle_size_val, cmap=cmap)\n",
    "ax.set_xticks(np.arange(min(keys_ratio_test), max(keys_ratio_test)+0.01, 0.04))\n",
    "ax.set_xlabel('PM2.5:PM10 Threshold')\n",
    "ax.set_ylabel('Mean NRMSE Between Three Models')\n",
    "cbar = fig.colorbar(color, ticks=[0.3, 0.5, 0.7, 0.9, 1.1, 1.3, 1.5])\n",
    "\n",
    "ax2.scatter(particle_size_val[:], nrmse_mean_sets[:], marker='o',label = 'Mean of 2 sets')\n",
    "# ax2.set_xticks(np.arange(min(x), max(x)+0.6, 0.2))\n",
    "ax2.set_xlabel('P10 Threshold')\n",
    "ax2.set_ylabel('Mean NRMSE Between Three Models')\n",
    "\n",
    "plt.suptitle('Mean NRMSE of Dust and Non-Dust ' + model_type + ' Models Across Thresholds --' +' Training: ' + training + '; Validation: ' + val +', '+ test + ' and ' + test2, y=0.92)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "    \n",
    "#### Plot ######\n",
    "fig = plt.figure(figsize=(16,6))\n",
    "ax = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "color = ax.scatter(keys_ratio_test[:], rmse_mean_sets[:], marker='o',label = 'Mean of 2 sets',\n",
    "                  c = particle_size_val, cmap=cmap)\n",
    "ax.set_xticks(np.arange(min(keys_ratio_test), max(keys_ratio_test)+0.01, 0.04))\n",
    "ax.set_xlabel('PM2.5:PM10 Threshold')\n",
    "ax.set_ylabel('Mean NRMSE Between Three Models')\n",
    "cbar = fig.colorbar(color, ticks=[0.3, 0.5, 0.7, 0.9, 1.1, 1.3, 1.5])\n",
    "\n",
    "ax2.scatter(particle_size_val[:], rmse_mean_sets[:], marker='o',label = 'Mean of 2 sets')\n",
    "# ax2.set_xticks(np.arange(min(x), max(x)+0.6, 0.2))\n",
    "ax2.set_xlabel('P10 Threshold')\n",
    "ax2.set_ylabel('Mean RMSE Between Three Models')\n",
    "\n",
    "plt.suptitle('Mean RMSE of Dust and Non-Dust ' + model_type + ' Models Across Thresholds --' +' Training: ' + training + '; Validation: ' + val +', '+ test + ' and ' + test2, y=0.92)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "    \n",
    "#### Plot ######\n",
    "fig = plt.figure(figsize=(16,6))\n",
    "ax = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "color = ax.scatter(keys_ratio_test[:], r2_mean_sets[:], marker='o',label = 'Mean of 2 sets',\n",
    "                  c = particle_size_val, cmap=cmap)\n",
    "ax.set_xticks(np.arange(min(keys_ratio_test), max(keys_ratio_test)+0.01, 0.04))\n",
    "ax.set_xlabel('PM2.5:PM10 Threshold')\n",
    "ax.set_ylabel('Mean NRMSE Between Three Models')\n",
    "cbar = fig.colorbar(color, ticks=[0.3, 0.5, 0.7, 0.9, 1.1, 1.3, 1.5])\n",
    "\n",
    "ax2.scatter(particle_size_val[:], r2_mean_sets[:], marker='o',label = 'Mean of 2 sets')\n",
    "# ax2.set_xticks(np.arange(min(x), max(x)+0.6, 0.2))\n",
    "ax2.set_xlabel('P10 Threshold')\n",
    "ax2.set_ylabel('Mean R2 Between Three Models')\n",
    "\n",
    "plt.suptitle('Mean R2 of Dust and Non-Dust ' + model_type + ' Models Across Thresholds --' +' Training: ' + training + '; Validation: ' + val +', '+ test + ' and ' + test2, y=0.92)\n",
    "# plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('NRMSE --','Training:', str(training), 'Validation:', str(val), nrmse_mean_ratio_val)\n",
    "# print('NRMSE --','Training:', str(training), 'Validation:', str(test), nrmse_mean_ratio_test)\n",
    "print('RMSE --','Training:', str(training), 'Validation:', str(val), rmse_mean_ratio_val)\n",
    "print('RMSE --','Training:', str(training), 'Validation:', str(test), rmse_mean_ratio_test)\n",
    "print('RMSE --','Training:', str(training), 'Validation:', str(test2), rmse_mean_ratio_test2)\n",
    "print('\\n')\n",
    "print('R2 --','Training:', str(training), 'Validation:', str(val), r2_mean_val)\n",
    "print('R2 --','Training:', str(training), 'Validation:', str(test), r2_mean_test)\n",
    "print('R2 --','Training:', str(training), 'Validation:', str(test2), r2_mean_test2)\n",
    "print('\\n')\n",
    "# print('Dust NRMSE:','Training:', str(training), 'Validation:', str(val),dust_nrmse_ratio_val)\n",
    "# print('Non-dust NRMSE:','Training:', str(training), 'Validation:', str(val),nodust_nrmse_ratio_val)\n",
    "print('Dust RMSE:','Training:', str(training), 'Validation:', str(val),dust_rmse_ratio_val)\n",
    "print('Non-dust RMSE:','Training:', str(training), 'Validation:', str(val),nodust_rmse_ratio_val)\n",
    "print('\\n')\n",
    "# print('Dust NRMSE:','Training:', str(training), 'Validation:', str(test),dust_nrmse_ratio_test)\n",
    "# print('Non-dust NRMSE:','Training:', str(training), 'Validation:', str(test),nodust_nrmse_ratio_test)\n",
    "print('Dust RMSE:','Training:', str(training), 'Validation:', str(test),dust_rmse_ratio_test)\n",
    "print('Non-dust RMSE:','Training:', str(training), 'Validation:', str(test),nodust_rmse_ratio_test)\n",
    "print('\\n')\n",
    "print('Dust RMSE:','Training:', str(training), 'Validation:', str(test2),dust_rmse_ratio_test2)\n",
    "print('Non-dust RMSE:','Training:', str(training), 'Validation:', str(test2),nodust_rmse_ratio_test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rmse_t1_v2.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(rmse_mean_ratio_val, filehandle)\n",
    "    \n",
    "with open('r2_t1_v2.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(r2_mean_val, filehandle)\n",
    "    \n",
    "with open('rmse_t1_v3.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(rmse_mean_ratio_test, filehandle)\n",
    "    \n",
    "with open('r2_t1_v3.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(r2_mean_test, filehandle)\n",
    "    \n",
    "with open('rmse_t1_v4.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(rmse_mean_ratio_test2, filehandle)\n",
    "    \n",
    "with open('r2_t1_v4.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(r2_mean_test2, filehandle)\n",
    "    \n",
    "with open('particle_sizes.data', 'wb') as filehandle:\n",
    "# store the data as binary data stream\n",
    "    pickle.dump(particle_size_val, filehandle)\n",
    "\n",
    "with open('ratios.data', 'wb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    pickle.dump(keys_ratio_val, filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation (Set 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rmse_t1_v2.data', 'rb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    rmse_t1_v2 = pickle.load(filehandle)\n",
    "    \n",
    "with open('r2_t1_v2.data', 'rb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    r2_t1_v2 = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 1 (Set 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rmse_t1_v3.data', 'rb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    rmse_t1_v3 = pickle.load(filehandle)\n",
    "\n",
    "with open('r2_t1_v3.data', 'rb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    r2_t1_v3 = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 2 (Set 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rmse_t1_v4.data', 'rb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    rmse_t1_v4 = pickle.load(filehandle)\n",
    "\n",
    "with open('r2_t1_v4.data', 'rb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    r2_t1_v4 = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('particle_sizes.data', 'rb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    particle_sizes = pickle.load(filehandle)\n",
    "\n",
    "with open('ratios.data', 'rb') as filehandle:\n",
    "    # store the data as binary data stream\n",
    "    ratios = pickle.load(filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dust Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_dust_sets = np.array([dust_mae_val,\n",
    "                            dust_mae_test,\n",
    "                            dust_mae_test2])\n",
    "mae_dust = np.average(mae_dust_sets, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmae_dust_sets = np.array([dust_nmae_val,\n",
    "                            dust_nmae_test,\n",
    "                            dust_nmae_test2])\n",
    "nmae_dust = np.average(nmae_dust_sets, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_dust_sets = np.array([dust_rmse_ratio_val,\n",
    "                            dust_rmse_ratio_test,\n",
    "                            dust_rmse_ratio_test2])\n",
    "rmse_dust = np.average(rmse_dust_sets, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_dust_sets = np.array([dust_r2_val,\n",
    "                    dust_r2_test,\n",
    "                    dust_r2_test2])\n",
    "r2_dust = np.average(r2_dust_sets, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dust = pd.DataFrame(\n",
    "    {'Particle Size': particle_sizes,\n",
    "    'Ratios':  ratios,\n",
    "     'R2':  r2_dust,\n",
    "     'RMSE': rmse_dust,\n",
    "     'MAE': mae_dust\n",
    "    })\n",
    "\n",
    "df_dustr2 = df_dust[['Ratios', 'Particle Size', 'R2']]\n",
    "dustr2_array = df_dustr2.to_numpy()\n",
    "\n",
    "df_dustmae = df_dust[['Ratios', 'Particle Size', 'MAE']]\n",
    "dustmae_array = df_dustmae.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dust.sort_values(by=['MAE'], ascending=True).reset_index(drop=True).to_csv(r'C:\\Users\\sph0088\\OneDrive - UNT System\\AQ\\Calibration_Sensors\\Collocation_Data\\test_data\\dust_mae2_rank.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dust.sort_values(by=['R2'], ascending=False).reset_index(drop=True).to_csv(r'C:\\Users\\sph0088\\OneDrive - UNT System\\AQ\\Calibration_Sensors\\Collocation_Data\\test_data\\dust_r22_rank.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Dust Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_nodust_sets = np.array([nodust_mae_val,\n",
    "                            nodust_mae_test,\n",
    "                            nodust_mae_test2])\n",
    "mae_nodust = np.average(mae_nodust_sets, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_nodust_sets = np.array([nodust_rmse_ratio_val,\n",
    "                            nodust_rmse_ratio_test,\n",
    "                            nodust_rmse_ratio_test2])\n",
    "rmse_nodust = np.average(rmse_nodust_sets, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_nodust_sets = np.array([nodust_r2_val,\n",
    "                    nodust_r2_test,\n",
    "                    nodust_r2_test2])\n",
    "r2_nodust = np.average(r2_nodust_sets, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nodust = pd.DataFrame(\n",
    "    {'Particle Size': particle_sizes,\n",
    "    'Ratios':  ratios,\n",
    "     'R2':  r2_nodust,\n",
    "     'MAE': mae_nodust\n",
    "    })\n",
    "\n",
    "df_nodustr2 = df_nodust[['Ratios', 'Particle Size', 'R2']]\n",
    "nodustr2_array = df_nodustr2.to_numpy()\n",
    "\n",
    "df_nodustmae = df_nodust[['Ratios', 'Particle Size', 'MAE']]\n",
    "nodustmae_array = df_nodustmae.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_dust.sort_values(by=['MAE'], ascending=True).reset_index(drop=True)\n",
    "df1.index.names = ['Rank1']\n",
    "df1 = df1.reset_index()\n",
    "# df1.to_csv(r'{}\\dust_mae2_rank.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_dust.sort_values(by=['R2'], ascending=False).reset_index(drop=True)\n",
    "df2.index.names = ['Rank2']\n",
    "df2 = df2.reset_index()\n",
    "# df2.to_csv(r'{}\\dust_r22_rank.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df_nodust.sort_values(by=['MAE'], ascending=True).reset_index(drop=True)\n",
    "df3.index.names = ['Rank3']\n",
    "df3 = df3.reset_index()\n",
    "# df3.to_csv(r'{}\\nodust_mae2_rank.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 =  df_nodust.sort_values(by=['R2'], ascending=False).reset_index(drop=True)\n",
    "df4.index.names = ['Rank4']\n",
    "df4 = df4.reset_index()\n",
    "# df4.to_csv(r'{}\\nodust_r22_rank.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge1 = pd.merge(df1, df2, on = ['Particle Size', 'Ratios'], how = 'outer')\n",
    "merge2 = pd.merge(merge1, df3, on = ['Particle Size', 'Ratios'], how = 'outer')\n",
    "merge3 = pd.merge(merge2, df4, on = ['Particle Size', 'Ratios'], how = 'outer')\n",
    "merge3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = merge3[['Particle Size', 'Ratios', 'Rank1', 'Rank2', 'Rank3', 'Rank4']]\n",
    "df5['Sum of Ranks'] = df5['Rank1'] + df5['Rank2'] + df5['Rank3'] + df5['Rank4'] \n",
    "df5.columns = ['P10 Count', 'PM2.5:PM10 Ratio', 'Dust MAE', 'Dust R2', 'Non-Dust MAE', 'Non-Dust R2', 'Sum of Ranks']\n",
    "# df5.sort_values(by=['Non-Dust MAE'], ascending=True)\n",
    "df5['Sum MAE'] =  df5['Dust MAE'] + df5['Non-Dust MAE']\n",
    "# df5.sort_values(by=['Sum MAE'], ascending=True).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Select Variables that Minimize Sum of Ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.sort_values(by=['Sum of Ranks'], ascending=True).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_nodust.sort_values(by=['MAE'], ascending=True).reset_index(drop=True).to_csv(r'C:\\Users\\sph0088\\OneDrive - UNT System\\AQ\\Calibration_Sensors\\Collocation_Data\\test_data\\nodust_mae2_rank.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_nodust.sort_values(by=['R2'], ascending=False).reset_index(drop=True).to_csv(r'C:\\Users\\sph0088\\OneDrive - UNT System\\AQ\\Calibration_Sensors\\Collocation_Data\\test_data\\nodust_r22_rank.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dust_val = df_dust_val[['tceq_pm25', 'pm2_5_atm_avg']]\n",
    "dust_val.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodust_val = df_nodust_val[['tceq_pm25', 'pm2_5_atm_avg']]\n",
    "nodust_val.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dust = df_dust[['tceq_pm25', 'pm2_5_atm_avg']]\n",
    "dust.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodust = df_nodust[['tceq_pm25', 'pm2_5_atm_avg']]\n",
    "nodust.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10.6/8.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tceq_pm25'].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
